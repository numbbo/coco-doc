

<!doctype html>


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>COCO: Comparing Continuous Optimizers &mdash; COCO: COmparing Continuous Optimizers</title>
    
    <link rel="stylesheet" href="_static/bizstyle.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '0.9',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="_static/bizstyle.js"></script>
    <link rel="top" title="COCO: COmparing Continuous Optimizers" href="#" />
    <meta name="viewport" content="width=device-width,initial-scale=1.0">
    <!--[if lt IE 9]>
    <script type="text/javascript" src="_static/css3-mediaqueries.js"></script>
    <![endif]-->
  </head>
  <body role="document">
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="nav-item nav-item-0"><a href="#">COCO: COmparing Continuous Optimizers</a> &raquo;</li> 
      </ul>
    </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="#">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">COCO: A platform for Comparing Continuous Optimizers in a Black-Box Setting</a><ul>
<li><a class="reference internal" href="#introduction">Introduction</a><ul>
<li><a class="reference internal" href="#why-coco">Why COCO?</a></li>
<li><a class="reference internal" href="#terminology">Terminology</a></li>
</ul>
</li>
<li><a class="reference internal" href="#functions-instances-problems-and-targets">Functions, Instances, Problems, and Targets</a></li>
<li><a class="reference internal" href="#runtime-and-target-values">Runtime and Target Values</a><ul>
<li><a class="reference internal" href="#restarts-and-simulated-restarts">Restarts and Simulated Restarts</a></li>
<li><a class="reference internal" href="#aggregation">Aggregation</a></li>
</ul>
</li>
<li><a class="reference internal" href="#general-code-structure">General Code Structure</a></li>
<li><a class="reference internal" href="#test-suites">Test Suites</a></li>
</ul>
</li>
</ul>

  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/index.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="coco-a-platform-for-comparing-continuous-optimizers-in-a-black-box-setting">
<h1>COCO: A platform for Comparing Continuous Optimizers in a Black-Box Setting<a class="headerlink" href="#coco-a-platform-for-comparing-continuous-optimizers-in-a-black-box-setting" title="Permalink to this headline">¶</a></h1>
See: <I>ArXiv e-prints</I>,
 <A HREF="http://arxiv.org/abs/1603.08785">arXiv:1603.08785</A>, 2016.<p><a class="reference external" href="https://github.com/numbbo/coco">COCO</a> is a platform for Comparing Continuous Optimizers in a black-box
setting.
It aims at automatizing the tedious and repetitive task of
benchmarking numerical optimization algorithms to the greatest possible
extent.
We present the rationals behind the development of the platform
as a general proposition for a guideline towards better benchmarking.
We detail underlying fundamental concepts of
<a class="reference external" href="https://github.com/numbbo/coco">COCO</a> such as its definition of
a problem, the idea of instances, the relevance of target values, and runtime
as central performance measure.
Finally, we  give a quick overview of the basic
code structure and the available test suites.</p>
<div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>We consider the continuous black-box optimization or search problem to minimize</p>
<div class="math">
<p><img src="_images/math/24791c42af3ae21a1f4b802374fd1c7cc7d997b2.png" alt="f: X\subset\mathbb{R}^n \to \mathbb{R}^m \qquad n,m\ge1"/></p>
</div><p>such that for the <img class="math" src="_images/math/c58e530eae38c5636539b02e8c92a338cc4cee12.png" alt="l" style="vertical-align: 0px"/> constraints</p>
<div class="math">
<p><img src="_images/math/45c8329e3bdc745a003418154b28c9c32fb3ef6d.png" alt="g: X\subset\mathbb{R}^n \to \mathbb{R}^l \qquad l\ge0"/></p>
</div><p>we have <img class="math" src="_images/math/ce6cd1adefd2891c416be6dc7ea4427b9c11da09.png" alt="g_i(\x)\le0" style="vertical-align: -4px"/> for all <img class="math" src="_images/math/3e903bbfeaf63c82eb7c86e73002505dbe6d3b20.png" alt="i=1\dots l" style="vertical-align: -1px"/>.
More specifically, we aim to find, as quickly as possible, one or several solutions <img class="math" src="_images/math/d1304f0b7773d247ff0207c9e036d817d003555b.png" alt="\x" style="vertical-align: 0px"/> in the search space <img class="math" src="_images/math/aa55e6775cdca96cc08ce73fa59a73133fd2d510.png" alt="X" style="vertical-align: 0px"/> with <em>small</em> value(s) of <img class="math" src="_images/math/5654ffc1269ad01c63d539d9c7c99d6dcfa7d039.png" alt="f(\x)\in\mathbb{R}^m" style="vertical-align: -4px"/> that satisfy all above constraints <img class="math" src="_images/math/8f6db44bda25a42ce182b1b908fe26305168addc.png" alt="g" style="vertical-align: -4px"/>.
We consider <em>time</em> to be defined as the number of calls to the function <img class="math" src="_images/math/f0307af130a9303d3def5b2c46714d19118bac36.png" alt="f" style="vertical-align: -4px"/>.</p>
<p>A continuous optimization algorithm, also known as <em>solver</em>, addresses the
above problem.
Here, we assume that <img class="math" src="_images/math/aa55e6775cdca96cc08ce73fa59a73133fd2d510.png" alt="X" style="vertical-align: 0px"/> is known, but no prior knowledge about <img class="math" src="_images/math/f0307af130a9303d3def5b2c46714d19118bac36.png" alt="f" style="vertical-align: -4px"/> or
<img class="math" src="_images/math/8f6db44bda25a42ce182b1b908fe26305168addc.png" alt="g" style="vertical-align: -4px"/> is available to the algorithm.
That is, <img class="math" src="_images/math/f0307af130a9303d3def5b2c46714d19118bac36.png" alt="f" style="vertical-align: -4px"/> and <img class="math" src="_images/math/8f6db44bda25a42ce182b1b908fe26305168addc.png" alt="g" style="vertical-align: -4px"/> are considered as a black-box which the algorithm can
query with solutions <img class="math" src="_images/math/8cc5181a2a64f34c353f32d2f4c7192934fac9f1.png" alt="\x\in\mathbb{R}^n" style="vertical-align: -1px"/> to get the respective values
<img class="math" src="_images/math/dc91e4306455a7fc24732bdd4a7f47c6bf7a07a5.png" alt="f(\x)" style="vertical-align: -4px"/> and <img class="math" src="_images/math/23426dc6e04a390b1cd17330c99cf38e9014834b.png" alt="g(\x)" style="vertical-align: -4px"/>.</p>
<p>From these prerequisits, benchmarking optimization algorithms seems to be a
rather simple and straightforward task. We run an algorithm on a collection of
problems and display the results. However, under closer inspection,
benchmarking turns out to be surprisingly tedious, and it appears to be
difficult to get results that can be meaningfully interpreted beyond the
standard claim that one algorithm is better than another on some problems. <a class="footnote-reference" href="#id7" id="id4">[1]</a>
Here, we offer a conceptual guideline for benchmarking
continuous optimization algorithms which tries to address this challenge and
has been implemented within the <a class="reference external" href="https://github.com/numbbo/coco">COCO</a> framework. <a class="footnote-reference" href="#id9" id="id5">[2]</a></p>
<p>The <a class="reference external" href="https://github.com/numbbo/coco">COCO</a> framework provides the practical means for an automatized
benchmarking procedure. Installing <a class="reference external" href="https://github.com/numbbo/coco">COCO</a> (in a shell) and benchmarking an
optimization algorithm, say, the function <code class="docutils literal"><span class="pre">fmin</span></code> from <code class="docutils literal"><span class="pre">scipy.optimize</span></code>
in Python, becomes as simple as</p>
<p><a class="footnote-reference" href="#id10" id="id6">[3]</a></p>
<div class="code bash highlight-python"><div class="highlight"><pre>$ ### get and install the code
$ git clone https://github.com/numbbo/coco.git  # get coco
$ cd coco
$ python do.py run-python  # install Python experimental module cocoex
$ python do.py install-postprocessing  # install post-processing :-)
</pre></div>
</div>
<div class="code bash highlight-python"><div class="highlight"><pre>$ ### (optional) run an example from the shell
$ cp code-experiments/build/python/example_experiment.py .
$ python example_experiment.py     # run the current &quot;default&quot; experiment
$ python -m bbob_pproc exdata/...  # run the post-processing
</pre></div>
</div>
<div class="code python highlight-python"><div class="highlight"><pre><span class="ch">#!/usr/bin/env python</span>
<span class="sd">&quot;&quot;&quot;Python script to benchmark fmin of scipy.optimize&quot;&quot;&quot;</span>
<span class="kn">import</span> <span class="nn">cocoex</span>
<span class="k">try</span><span class="p">:</span> <span class="kn">import</span> <span class="nn">cocopp</span>  <span class="c1"># new (future) name</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span> <span class="kn">import</span> <span class="nn">bbob_pproc</span> <span class="kn">as</span> <span class="nn">cocopp</span>  <span class="c1"># old name</span>
<span class="kn">from</span> <span class="nn">scipy.optimize</span> <span class="kn">import</span> <span class="n">fmin</span>

<span class="n">suite</span> <span class="o">=</span> <span class="n">cocoex</span><span class="o">.</span><span class="n">Suite</span><span class="p">(</span><span class="s2">&quot;bbob&quot;</span><span class="p">,</span> <span class="s2">&quot;year: 2016&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
<span class="n">observer</span> <span class="o">=</span> <span class="n">cocoex</span><span class="o">.</span><span class="n">Observer</span><span class="p">(</span><span class="s2">&quot;bbob&quot;</span><span class="p">,</span> <span class="s2">&quot;result_folder: myoptimizer-on-bbob&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">suite</span><span class="p">:</span>  <span class="c1"># loop over all problems</span>
    <span class="n">observer</span><span class="o">.</span><span class="n">observe</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>  <span class="c1"># prepare logging of necessary data</span>
    <span class="n">fmin</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">initial_solution</span><span class="p">)</span>  <span class="c1"># disp=False would silence fmin output</span>

<span class="n">cocopp</span><span class="o">.</span><span class="n">main</span><span class="p">(</span><span class="s1">&#39;exdata/myoptimizer-on-bbob&#39;</span><span class="p">)</span>  <span class="c1"># invoke data post-processing</span>
</pre></div>
</div>
<p>Now the file <code class="docutils literal"><span class="pre">ppdata/ppdata.html</span></code> can be used to browse the resulting data.</p>
<p>The <a class="reference external" href="https://github.com/numbbo/coco">COCO</a> framework provides</p>
<blockquote>
<div><ul class="simple">
<li>an interface to several languages in which the benchmarked optimizer
can be written, currently C/C++, Java, Matlab/Octave, Python</li>
<li>several benchmark suites or testbeds, currently all written in C</li>
<li>data logging facilities via the <code class="docutils literal"><span class="pre">Observer</span></code></li>
<li>data post-processing in Python and data display facilities in <code class="docutils literal"><span class="pre">html</span></code></li>
<li>article LaTeX templates</li>
</ul>
</div></blockquote>
<p>The underlying philosophy of <a class="reference external" href="https://github.com/numbbo/coco">COCO</a> is to provide everything which otherwise
most experimenters needed to setup and implement themselves, if they wanted to
benchmark an algorithm <em>properly</em>. So far, the framework has been used successfully for
benchmarking far over a hundred algorithms by many researchers.</p>
<table class="docutils footnote" frame="void" id="id7" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id4">[1]</a></td><td>One major flaw is that we often get no
indication of <em>how much</em> better an algorithm is.
That is, the results of benchmarking often provide no indication of
<em>relevance</em>;
the main output often consists of hundreds of tabulated numbers
only interpretable on an <em>ordinal scale</em> <a class="reference internal" href="#ste1946" id="id8">[STE1946]</a>. Addressing a point of a
common confusion, <em>statistical significance</em> is only a secondary, and by no
means a <em>sufficient</em> condition for <em>relevance</em>.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id9" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id5">[2]</a></td><td>See <a class="reference external" href="https://www.github.com/numbbo/coco">https://www.github.com/numbbo/coco</a> or <a class="reference external" href="https://numbbo.github.io">https://numbbo.github.io</a> for implementation details.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id10" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id6">[3]</a></td><td>In order to get even more insightful result on the comparatively
difficult <code class="docutils literal"><span class="pre">bbob</span></code> benchmark suite, additionally randomized restarts are advisable.
Example code is given in <a class="reference external" href="https://github.com/numbbo/coco/blob/master/code-experiments/build/python/example_experiment.py"><code class="docutils literal"><span class="pre">example_experiment.py</span></code></a> which runs
out-of-the-box as a benchmarking Python script.</td></tr>
</tbody>
</table>
<div class="section" id="why-coco">
<h3>Why <a class="reference external" href="https://github.com/numbbo/coco">COCO</a>?<a class="headerlink" href="#why-coco" title="Permalink to this headline">¶</a></h3>
<p>Appart from diminishing the burden (time) and the pitfalls, bugs
or omissions of the repetitive coding task for many experimenters, our aim is to
provide a <em>conceptual guideline for better benchmarking</em>. Our guideline has
the following defining features.</p>
<ol class="arabic">
<li><p class="first">Benchmark functions are</p>
<ol class="arabic simple">
<li>used as black boxes for the algorithm, however they
are explicitly known to the scientific community.</li>
<li>designed to be comprehensible, to allow a meaningful
interpretation of performance results.</li>
<li>difficult to &#8220;defeat&#8221;, that is, they do not
have artificial regularities that can easily be (intentionally or unintentionally)
exploited by an algorithm. <a class="footnote-reference" href="#id18" id="id11">[4]</a></li>
<li>scalable with the input dimension <a class="reference internal" href="#whi1996" id="id12">[WHI1996]</a>.</li>
</ol>
</li>
<li><p class="first">There is no predefined budget (number of <img class="math" src="_images/math/f0307af130a9303d3def5b2c46714d19118bac36.png" alt="f" style="vertical-align: -4px"/>-evaluations) for running an
experiment, the experimental procedure is <em>budget-free</em> <a class="reference internal" href="#han2016ex" id="id13">[HAN2016ex]</a>.</p>
</li>
<li><p class="first">A single performance measure is used &#8212; and thereafter aggregated and
displayed in
several ways &#8212; namely <strong>runtime</strong>, <em>measured in
number of</em> <img class="math" src="_images/math/f0307af130a9303d3def5b2c46714d19118bac36.png" alt="f" style="vertical-align: -4px"/>-<em>evaluations</em> <a class="reference internal" href="#han2016perf" id="id14">[HAN2016perf]</a>. Runtime has the advantages to</p>
<ul class="simple">
<li>be independent of the computational platform, language, compiler, coding
styles, and other specific experimental conditions <a class="footnote-reference" href="#id20" id="id15">[5]</a></li>
<li>be relevant, meaningful and easily interpretable without expert domain knowledge</li>
<li>be quantitative on the ratio scale <a class="reference internal" href="#ste1946" id="id16">[STE1946]</a> <a class="footnote-reference" href="#id22" id="id17">[6]</a></li>
<li>assume a wide range of values</li>
<li>aggregate over a collection of values in a meaningful way</li>
</ul>
<p>A <em>missing</em> runtime value is considered as possible outcome (see below).</p>
</li>
<li><p class="first">The display is as comprehensible, intuitive and informative as possible.
We believe that details matter.
Aggregation over dimension is avoided, because dimension is an a priori
known parameter that can and should be used for algorithm design or selection
decisions.
This is possible without significant drawbacks, because all functions are
scalable in the dimension.</p>
</li>
</ol>
<table class="docutils footnote" frame="void" id="id18" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id11">[4]</a></td><td>For example, the optimum is not in all-zeros, optima are not placed
on a regular grid, most functions are not separable <a class="reference internal" href="#whi1996" id="id19">[WHI1996]</a>. The
objective to remain comprehensible makes it more challenging to design
non-regular functions. Which regularities are common place in real-world
optimization problems remains an open question.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id20" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id15">[5]</a></td><td>Runtimes measured in <img class="math" src="_images/math/f0307af130a9303d3def5b2c46714d19118bac36.png" alt="f" style="vertical-align: -4px"/>-evaluations are widely
comparable and designed to stay. The experimental procedure
<a class="reference internal" href="#han2016ex" id="id21">[HAN2016ex]</a> includes however a timing experiment which records the
internal computational effort of the algorithm in CPU or wall clock time.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id22" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id17">[6]</a></td><td>As opposed to a ranking of algorithm based on their solution quality
achieved after a given budget.</td></tr>
</tbody>
</table>
</div>
<div class="section" id="terminology">
<h3>Terminology<a class="headerlink" href="#terminology" title="Permalink to this headline">¶</a></h3>
<p>We specify a few terms which are used later.</p>
<dl class="docutils">
<dt><em>function</em></dt>
<dd>We talk about an objective <em>function</em> as a parametrized mapping
<img class="math" src="_images/math/bb3b4cc088220f561d9624697122d49cf057f0b2.png" alt="\mathbb{R}^n\to\mathbb{R}^m" style="vertical-align: -1px"/> with scalable input space, <img class="math" src="_images/math/7ac2cdf2ea041185339441fef74e963993bc930a.png" alt="n&gt;\ge2" style="vertical-align: -3px"/>,
and usually <img class="math" src="_images/math/64a1faffe7792d85f77af7923e09a5c0b82bb1fc.png" alt="m\in\{1,2\}" style="vertical-align: -5px"/>.
Functions are parametrized such that different <em>instances</em> of the
&#8220;same&#8221; function are available, e.g. translated or shifted versions.</dd>
<dt><em>problem</em></dt>
<dd>We talk about a <em>problem</em>, <a class="reference external" href="http://numbbo.github.io/coco-doc/C/coco_8h.html#a408ba01b98c78bf5be3df36562d99478"><code class="docutils literal"><span class="pre">coco_problem_t</span></code></a>, as a specific <em>function
instance</em> on which an optimization algorithm is run.
A problem
can be evaluated and returns an <img class="math" src="_images/math/f0307af130a9303d3def5b2c46714d19118bac36.png" alt="f" style="vertical-align: -4px"/>-value or -vector and, in case,
a <img class="math" src="_images/math/8f6db44bda25a42ce182b1b908fe26305168addc.png" alt="g" style="vertical-align: -4px"/>-vector.
In the context of performance assessment, a target <img class="math" src="_images/math/f0307af130a9303d3def5b2c46714d19118bac36.png" alt="f" style="vertical-align: -4px"/>- or
indicator-value is added to define a problem.</dd>
<dt><em>runtime</em></dt>
<dd>We define <em>runtime</em>, or <em>run-length</em> <a class="reference internal" href="#hoo1998" id="id23">[HOO1998]</a> as the <em>number of
evaluations</em> conducted on a given problem until a prescribed target value is
hit, also referred to as number of <em>function</em> evaluations or <img class="math" src="_images/math/f0307af130a9303d3def5b2c46714d19118bac36.png" alt="f" style="vertical-align: -4px"/>-evaluations.
Runtime is our central performance measure.</dd>
<dt><em>suite</em></dt>
<dd>A test- or benchmark-suite is a collection of problems, typically between
twenty and a hundred, where the number of objectives <img class="math" src="_images/math/862c9e331021da02c140ce6caef6f1d6519f571c.png" alt="m" style="vertical-align: 0px"/> is fixed.</dd>
</dl>
</div>
</div>
<div class="section" id="functions-instances-problems-and-targets">
<h2>Functions, Instances, Problems, and Targets<a class="headerlink" href="#functions-instances-problems-and-targets" title="Permalink to this headline">¶</a></h2>
<p>In the <a class="reference external" href="https://github.com/numbbo/coco">COCO</a> framework we consider <strong>functions</strong>, <img class="math" src="_images/math/79f3bb3d79a8de41a5a700d22d90d16bda92171d.png" alt="f_i" style="vertical-align: -4px"/>, for each suite
distinguished by their identifier <img class="math" src="_images/math/4d7143c57d4cf8653a2aa6b4c3c6c97446fc1592.png" alt="i=1,2,\dots" style="vertical-align: -4px"/> .
Functions are further
<em>parametrized</em> by the (input) dimension, <img class="math" src="_images/math/273f486f7ce4023c108fff2c278487a34aaa6321.png" alt="n" style="vertical-align: 0px"/>, and the instance number, <img class="math" src="_images/math/82ae6a6921ae07dd0e556d8402502aaea53a96d4.png" alt="j" style="vertical-align: -4px"/>, <a class="footnote-reference" href="#id27" id="id24">[7]</a>
that is, for a given <img class="math" src="_images/math/862c9e331021da02c140ce6caef6f1d6519f571c.png" alt="m" style="vertical-align: 0px"/> we have</p>
<div class="math">
<p><img src="_images/math/870d97f75649775ba4ea7c242dd34e37f50be4c4.png" alt="\finstance_i \equiv f(n, i, j):\R^n \to \mathbb{R}^m \quad
\x \mapsto \finstance_i (\x) = f(n, i, j)(\x)\enspace."/></p>
</div><p>Varying <img class="math" src="_images/math/273f486f7ce4023c108fff2c278487a34aaa6321.png" alt="n" style="vertical-align: 0px"/> or <img class="math" src="_images/math/82ae6a6921ae07dd0e556d8402502aaea53a96d4.png" alt="j" style="vertical-align: -4px"/> leads to a variation of the same function
<img class="math" src="_images/math/fc410bef70dece978b90fa5bb68c683360db785b.png" alt="i" style="vertical-align: 0px"/> of a given suite.
By fixing <img class="math" src="_images/math/273f486f7ce4023c108fff2c278487a34aaa6321.png" alt="n" style="vertical-align: 0px"/> and <img class="math" src="_images/math/82ae6a6921ae07dd0e556d8402502aaea53a96d4.png" alt="j" style="vertical-align: -4px"/> for function <img class="math" src="_images/math/79f3bb3d79a8de41a5a700d22d90d16bda92171d.png" alt="f_i" style="vertical-align: -4px"/>, we define an optimization <strong>problem</strong>
<img class="math" src="_images/math/6677d043e7a3bbb44f3c74922a5efc455f7f0730.png" alt="(n, i, j)\equiv(f_i, n, j)" style="vertical-align: -4px"/> that can be presented to the optimization algorithm. Each problem receives again
an index in the suite, mapping the triple <img class="math" src="_images/math/558b03aade774fffd5caced4fd5e81fb9bcc8f1b.png" alt="(n, i, j)" style="vertical-align: -4px"/> to a single
number.</p>
<p>As the formalization above suggests, the differentiation between function (index)
and instance index is of purely semantic nature.
This semantics however is important in how we display and
interpret the results. We interpret <strong>varying the instance</strong> parameter as
a natural randomization for experiments <a class="footnote-reference" href="#id28" id="id25">[8]</a> in order to</p>
<blockquote>
<div><ul>
<li><p class="first">generate repetitions on a function and</p>
</li>
<li><dl class="first docutils">
<dt>average away irrelevant aspects of a function thereby providing</dt>
<dd><ul class="first last simple">
<li>generality which alleviates the problem of overfitting, and</li>
<li>a fair setup which prevents intentional or unintentional exploitation of
irrelevant or artificial function properties.</li>
</ul>
</dd>
</dl>
</li>
</ul>
</div></blockquote>
<p>For example, we consider the absolute location of the optimum not a defining
function feature. Consequently, in a typical <a class="reference external" href="https://github.com/numbbo/coco">COCO</a> benchmark suite, instances
with randomized search space translations are presented to the optimizer. <a class="footnote-reference" href="#id29" id="id26">[9]</a></p>
<table class="docutils footnote" frame="void" id="id27" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id24">[7]</a></td><td>We can think of <img class="math" src="_images/math/82ae6a6921ae07dd0e556d8402502aaea53a96d4.png" alt="j" style="vertical-align: -4px"/> as an index to a continuous parameter vector setting,
as it parametrizes, among others things, translations and rotations. In
practice, <img class="math" src="_images/math/82ae6a6921ae07dd0e556d8402502aaea53a96d4.png" alt="j" style="vertical-align: -4px"/> is the discrete identifier for single instantiations of
these parameters.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id28" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id25">[8]</a></td><td>Changing or sweeping through a relevant feature of the problem class,
systematically or randomized, is another possible usage of instance
parametrization.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id29" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id26">[9]</a></td><td>Conducting either several trials on instances with randomized search space
translations or with a randomized initial solution is equivalent, given
that the optimizer behaves translation invariant (disregarding domain
boundaries).</td></tr>
</tbody>
</table>
</div>
<div class="section" id="runtime-and-target-values">
<h2>Runtime and Target Values<a class="headerlink" href="#runtime-and-target-values" title="Permalink to this headline">¶</a></h2>
<p>In order to measure the runtime of an algorithm on a problem, we
establish a hitting time condition.
We prescribe a <strong>target value</strong>, <img class="math" src="_images/math/dc02ab1d58825bc03d8f3711007c26c0beb55232.png" alt="t" style="vertical-align: 0px"/>, which is an <img class="math" src="_images/math/f0307af130a9303d3def5b2c46714d19118bac36.png" alt="f" style="vertical-align: -4px"/>- or
indicator-value <a class="reference internal" href="#han2016perf" id="id30">[HAN2016perf]</a> <a class="reference internal" href="#tus2016" id="id31">[TUS2016]</a>.
For a single run, when an algorithm reaches or surpasses the target value <img class="math" src="_images/math/dc02ab1d58825bc03d8f3711007c26c0beb55232.png" alt="t" style="vertical-align: 0px"/>
on problem <img class="math" src="_images/math/301c64935fc6bdae853a720871286bccf6cae341.png" alt="(f_i, n, j)" style="vertical-align: -4px"/>, we say it has <em>solved the problem</em> <img class="math" src="_images/math/6abd084e25cb7cb33a31d01b8f76f4382506956c.png" alt="(f_i, n, j, t)" style="vertical-align: -4px"/> &#8212; it was successful. <a class="footnote-reference" href="#id36" id="id32">[10]</a></p>
<p>Now, the <strong>runtime</strong> is the evaluation count when the target value <img class="math" src="_images/math/dc02ab1d58825bc03d8f3711007c26c0beb55232.png" alt="t" style="vertical-align: 0px"/> was
reached or surpassed for the first time.
That is, runtime is the number of <img class="math" src="_images/math/f0307af130a9303d3def5b2c46714d19118bac36.png" alt="f" style="vertical-align: -4px"/>-evaluations needed to solve the problem
<img class="math" src="_images/math/6abd084e25cb7cb33a31d01b8f76f4382506956c.png" alt="(f_i, n, j, t)" style="vertical-align: -4px"/> (but see also <a class="reference external" href="https://www.github.com">Recommendations</a> in <a class="reference internal" href="#han2016ex" id="id33">[HAN2016ex]</a>). <a class="footnote-reference" href="#id37" id="id34">[11]</a>
<em>Measured runtimes are the only way of how we assess the performance of an
algorithm.</em> <a class="footnote-reference" href="#id39" id="id35">[12]</a></p>
<p>If an algorithm does not hit the target in a single run, the runtime remains
undefined &#8212; while
it has been bound to be at least <img class="math" src="_images/math/9fd6cf5de229474507de3a7f3164ce2c6e0d27c0.png" alt="k+1" style="vertical-align: -1px"/>, where <img class="math" src="_images/math/93ad71e1e72170c026513e0449297ebab47bec72.png" alt="k" style="vertical-align: 0px"/> is the number of
evaluations in this unsuccessful run.
The number of defined runtime values depends on the budget the
algorithm has explored.
Therefore, larger budgets are preferable &#8212; however they should not come at
the expense of abandoning reasonable termination conditions. Instead,
restarts should be done.</p>
<table class="docutils footnote" frame="void" id="id36" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id32">[10]</a></td><td>Note the use of the term <em>problem</em> in two meanings: as the problem the
algorithm is benchmarked on, <img class="math" src="_images/math/301c64935fc6bdae853a720871286bccf6cae341.png" alt="(f_i, n, j)" style="vertical-align: -4px"/>, and as the problem, <img class="math" src="_images/math/6abd084e25cb7cb33a31d01b8f76f4382506956c.png" alt="(f_i, n, j, t)" style="vertical-align: -4px"/>, an algorithm can
solve by hitting the target <img class="math" src="_images/math/dc02ab1d58825bc03d8f3711007c26c0beb55232.png" alt="t" style="vertical-align: 0px"/> with the runtime, <img class="math" src="_images/math/29a2a393300437c22847c898bec704867934d5e9.png" alt="\mathrm{RT}(f_i, n, j, t)" style="vertical-align: -4px"/>, or may fail to solve.
Each problem <img class="math" src="_images/math/301c64935fc6bdae853a720871286bccf6cae341.png" alt="(f_i, n, j)" style="vertical-align: -4px"/> gives raise to a collection of dependent problems <img class="math" src="_images/math/6abd084e25cb7cb33a31d01b8f76f4382506956c.png" alt="(f_i, n, j, t)" style="vertical-align: -4px"/>.
Viewed as random variables, the events <img class="math" src="_images/math/29a2a393300437c22847c898bec704867934d5e9.png" alt="\mathrm{RT}(f_i, n, j, t)" style="vertical-align: -4px"/> given <img class="math" src="_images/math/301c64935fc6bdae853a720871286bccf6cae341.png" alt="(f_i, n, j)" style="vertical-align: -4px"/> are not
independent events for different values of <img class="math" src="_images/math/dc02ab1d58825bc03d8f3711007c26c0beb55232.png" alt="t" style="vertical-align: 0px"/>.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id37" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id34">[11]</a></td><td>Target values are directly linked to a problem, leaving the burden to
properly define the targets with the designer of the benchmark suite.
The alternative is to present final <img class="math" src="_images/math/f0307af130a9303d3def5b2c46714d19118bac36.png" alt="f" style="vertical-align: -4px"/>- or indicator-values as results,
leaving the (rather unsurmountable) burden to interpret these values to the
reader.
Fortunately, there is an automatized generic way to generate target values
from observed runtimes, the so-called run-length based target values
<a class="reference internal" href="#han2016perf" id="id38">[HAN2016perf]</a>.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id39" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id35">[12]</a></td><td>Observed success rates can (and should) be translated into lower bounds
on runtimes on a subset of problems.</td></tr>
</tbody>
</table>
<div class="section" id="restarts-and-simulated-restarts">
<span id="sec-restarts"></span><h3>Restarts and Simulated Restarts<a class="headerlink" href="#restarts-and-simulated-restarts" title="Permalink to this headline">¶</a></h3>
<p>An optimization algorithm is bound to terminate and, in the single-objective case, return a recommended
solution, <img class="math" src="_images/math/d1304f0b7773d247ff0207c9e036d817d003555b.png" alt="\x" style="vertical-align: 0px"/>, for the problem, <img class="math" src="_images/math/301c64935fc6bdae853a720871286bccf6cae341.png" alt="(f_i, n, j)" style="vertical-align: -4px"/>.
It solves thereby all problems <img class="math" src="_images/math/6abd084e25cb7cb33a31d01b8f76f4382506956c.png" alt="(f_i, n, j, t)" style="vertical-align: -4px"/> for which <img class="math" src="_images/math/9132780939852d3f51a824c5ac4fecc302013d72.png" alt="f(\x)\le t" style="vertical-align: -4px"/>.
Independent restarts from different, randomized initial solutions are a simple
but powerful tool to increase the number of solved problems <a class="reference internal" href="#har1999" id="id40">[HAR1999]</a> &#8212; namely by increasing the number of <img class="math" src="_images/math/dc02ab1d58825bc03d8f3711007c26c0beb55232.png" alt="t" style="vertical-align: 0px"/>-values, for which the problem <img class="math" src="_images/math/301c64935fc6bdae853a720871286bccf6cae341.png" alt="(f_i, n, j)" style="vertical-align: -4px"/>
was solved. <a class="footnote-reference" href="#id48" id="id41">[13]</a>
Independent restarts tend to increase the success rate, but they generally do
not <em>change</em> the performance <em>assessment</em>, because the successes materialize at
greater runtimes.
Therefore, we call our approach <em>budget-free</em>.
Restarts however &#8220;<em>improve the reliability, comparability, precision, and &#8220;visibility&#8221; of the measured results</em>&#8221; <a class="reference internal" href="#han2016ex" id="id42">[HAN2016ex]</a>.</p>
<p><em>Simulated restarts</em> <a class="reference internal" href="#han2010ex" id="id43">[HAN2010ex]</a> <a class="reference internal" href="#han2010" id="id44">[HAN2010]</a> <a class="reference internal" href="#han2016perf" id="id45">[HAN2016perf]</a> are used to determine a runtime for unsuccessful runs. Semantically, this is only valid if we interpret different
instances as random repetitions.
Resembling the bootstrapping method <a class="reference internal" href="#efr1994" id="id46">[EFR1994]</a>, when we face an unsolved problem, we draw uniformly at random a
new <img class="math" src="_images/math/82ae6a6921ae07dd0e556d8402502aaea53a96d4.png" alt="j" style="vertical-align: -4px"/> until we find an instance such that <img class="math" src="_images/math/6abd084e25cb7cb33a31d01b8f76f4382506956c.png" alt="(f_i, n, j, t)" style="vertical-align: -4px"/> was solved. <a class="footnote-reference" href="#id49" id="id47">[14]</a>
The evaluations done on the first unsolved problem and on all subsequently
drawn unsolved problems are added to the runtime on the last problem and
are considered as runtime on the original unsolved problem.
This method is applied if a problem instance was not solved and is
(only) available if at least one problem instance was solved.</p>
<table class="docutils footnote" frame="void" id="id48" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id41">[13]</a></td><td>For a given problem <img class="math" src="_images/math/301c64935fc6bdae853a720871286bccf6cae341.png" alt="(f_i, n, j)" style="vertical-align: -4px"/>, the number of acquired runtime values, <img class="math" src="_images/math/29a2a393300437c22847c898bec704867934d5e9.png" alt="\mathrm{RT}(f_i, n, j, t)" style="vertical-align: -4px"/>
is monotonously increasing with the budget used. Considered as random
variables, these runtimes are not independent.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id49" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id47">[14]</a></td><td>More specifically, we consider the problems <img class="math" src="_images/math/ae95f31e6a988bc22c9d77572fc9468269b004f6.png" alt="(f_i, n, j, t(j))" style="vertical-align: -4px"/> for
all benchmarked instances <img class="math" src="_images/math/82ae6a6921ae07dd0e556d8402502aaea53a96d4.png" alt="j" style="vertical-align: -4px"/>. The targets <img class="math" src="_images/math/bcfd2d20d1baefb0af5e251bb6f3e3a9e4e9a928.png" alt="t(j)" style="vertical-align: -4px"/> depend on the instance
in a way to make the problems comparable <a class="reference internal" href="#han2016perf" id="id50">[HAN2016perf]</a>.</td></tr>
</tbody>
</table>
</div>
<div class="section" id="aggregation">
<h3>Aggregation<a class="headerlink" href="#aggregation" title="Permalink to this headline">¶</a></h3>
<p>A typical benchmark suite consists of about 20&#8211;100 functions with 5&#8211;15 instances for each function. For each instance, up to about 100 targets are considered for the
performance assessment. This means we want to consider at least <img class="math" src="_images/math/d5cebea34deef8a28fa883338aa8bb10f9629976.png" alt="20\times5=100" style="vertical-align: -1px"/>, and
up to <img class="math" src="_images/math/16ec9066ffdaaab3f0437d90ad41128e491c406f.png" alt="100\times15\times100=150\,000" style="vertical-align: -1px"/> runtimes for the performance assessment.
To make them amenable to the experimenter, we need to summarize these data.</p>
<p>Our idea behind an aggregation is to make a statistical summary over a set or
subset of <em>problems of interest</em> over which we assume a uniform distribution.
From a practical perspective this means to have no simple way to distinguish
between these problems and to select an optimization algorithm accordingly&#8212;in
which case an aggregation would have no significance&#8212;and that we are likely
to face each problem with similar probability.
We do not aggregate over dimension, because dimension can and
should be used for algorithm selection.</p>
<p>We have several ways to aggregate the resulting runtimes.</p>
<blockquote>
<div><ul class="simple">
<li>Empirical distribution functions (ECDF). In the domain of
optimization, ECDF are also known as <em>data profiles</em> <a class="reference internal" href="#mor2009" id="id51">[MOR2009]</a>. We
prefer the simple ECDF over the more innovative performance profiles
<a class="reference internal" href="#mor2002" id="id52">[MOR2002]</a> for two reasons.
ECDF (i) do not depend on other presented algorithms, that is, they are
entirely comparable across different publications, and (ii) let us distinguish in a
natural way easy problems from difficult problems for the considered
algorithm. <a class="footnote-reference" href="#id56" id="id53">[15]</a> We usually display ECDF on the log scale, which makes the area
above the curve and the <em>difference area</em> between two curves a meaningful
conception <a class="reference internal" href="#han2016perf" id="id54">[HAN2016perf]</a>.</li>
<li>Averaging, as an estimator of the expected runtime. The average runtime, that
is the estimated expected runtime, is
often plotted against dimension to indicate scaling with dimension. The
<em>arithmetic</em> average
is only meaningful if the underlying distribution of the values
is similar. Otherwise, the average of log-runtimes, or <em>geometric</em> average,
is useful.</li>
<li>Restarts and simulated restarts, see Section <a class="reference internal" href="#sec-restarts"><span>Restarts and Simulated Restarts</span></a>, do not
literally aggregate runtimes (which are literally defined only when <img class="math" src="_images/math/dc02ab1d58825bc03d8f3711007c26c0beb55232.png" alt="t" style="vertical-align: 0px"/> was
hit).  They aggregate, however, time data to eventually supplement missing runtime
values, see also <a class="reference internal" href="#han2016perf" id="id55">[HAN2016perf]</a>.</li>
</ul>
</div></blockquote>
<table class="docutils footnote" frame="void" id="id56" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id53">[15]</a></td><td>When reading a performance profile, an invariable immediate question is
often whether a large runtime difference is mainly due to one algorithm solving
the problem very quickly. This question cannot be answered from the profile.
The advantage (i) over data profiles disappears when using run-length based
target values <a class="reference internal" href="#han2016perf" id="id57">[HAN2016perf]</a>.</td></tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="general-code-structure">
<h2>General Code Structure<a class="headerlink" href="#general-code-structure" title="Permalink to this headline">¶</a></h2>
<p>The code basis of the <a class="reference external" href="https://github.com/numbbo/coco">COCO</a> code consists of two parts.</p>
<dl class="docutils">
<dt>The <em>experiments</em> part</dt>
<dd>defines test suites, allows to conduct experiments, and provides the output
data. The <a class="reference external" href="http://numbbo.github.io/coco-doc/C">code base is written in C</a>, and wrapped in different languages
(currently Java, Python, Matlab/Octave). An amalgamation technique is used
that outputs two files <code class="docutils literal"><span class="pre">coco.h</span></code> and <code class="docutils literal"><span class="pre">coco.c</span></code> which suffice to run
experiments within the <a class="reference external" href="https://github.com/numbbo/coco">COCO</a> framework.</dd>
<dt>The <em>post-processing</em> part</dt>
<dd>processes the data and displays the resulting runtimes. This part is
entirely written in Python and heavily depends on <a class="reference external" href="http://matplotlib.org/"><code class="docutils literal"><span class="pre">matplotlib</span></code></a> <a class="reference internal" href="#hun2007" id="id59">[HUN2007]</a>.</dd>
</dl>
</div>
<div class="section" id="test-suites">
<h2>Test Suites<a class="headerlink" href="#test-suites" title="Permalink to this headline">¶</a></h2>
<p>Currently, the <a class="reference external" href="https://github.com/numbbo/coco">COCO</a> framework provides three different test suites.</p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">bbob</span></code></dt>
<dd>contains 24 functions in five subgroups <a class="reference internal" href="#han2009fun" id="id60">[HAN2009fun]</a>.</dd>
<dt><code class="docutils literal"><span class="pre">bbob-noisy</span></code></dt>
<dd>contains 30 noisy problems in three subgroups <a class="reference internal" href="#han2009noi" id="id61">[HAN2009noi]</a>,
currently only implemented in the <a class="reference external" href="http://coco.gforge.inria.fr/doku.php?id=downloads">old code basis</a>.</dd>
<dt><code class="docutils literal"><span class="pre">bbob-biobj</span></code></dt>
<dd>contains 55 bi-objective (<img class="math" src="_images/math/35a82f7fa7e77be3728336c5bd59dc655eb19b57.png" alt="m=2" style="vertical-align: 0px"/>) functions in 15 subgroups <a class="reference internal" href="#tus2016" id="id62">[TUS2016]</a>.</dd>
</dl>
<H2>Acknowledgments</H2><p>The authors would like to thank Raymond Ros, Steffen Finck, Marc Schoenauer,
Petr Posik and Dejan Tušar for their many invaluable contributions to this work.</p>
<p>The authors also acknowledge support by the grant ANR-12-MONU-0009 (NumBBO)
of the French National Research Agency.</p>
<H2>References</H2><table class="docutils citation" frame="void" id="han2016perf" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[HAN2016perf]</td><td><em>(<a class="fn-backref" href="#id14">1</a>, <a class="fn-backref" href="#id30">2</a>, <a class="fn-backref" href="#id38">3</a>, <a class="fn-backref" href="#id45">4</a>, <a class="fn-backref" href="#id50">5</a>, <a class="fn-backref" href="#id54">6</a>, <a class="fn-backref" href="#id55">7</a>, <a class="fn-backref" href="#id57">8</a>)</em> N. Hansen, A. Auger, D. Brockhoff, D. Tušar, T. Tušar (2016).
<a class="reference external" href="http://numbbo.github.io/coco-doc/perf-assessment">COCO: Performance Assessment</a>. <em>ArXiv e-prints</em>, <a class="reference external" href="http://arxiv.org/abs/1605.03560">arXiv:1605.03560</a></td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="han2010ex" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id43">[HAN2010ex]</a></td><td>N. Hansen, A. Auger, S. Finck, and R. Ros (2010).
Real-Parameter Black-Box Optimization Benchmarking 2010: Experimental Setup, <a class="reference external" href="http://hal.inria.fr/inria-00362649/en">Research Report RR-7215</a>, Inria.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="han2010" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id44">[HAN2010]</a></td><td>N. Hansen, A. Auger, R. Ros, S. Finck, and P. Posik (2010).
Comparing Results of 31 Algorithms from the Black-Box Optimization Benchmarking BBOB-2009. Workshop Proceedings of the GECCO Genetic and Evolutionary Computation Conference 2010, ACM, pp. 1689-1696</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="han2009fun" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id60">[HAN2009fun]</a></td><td>N. Hansen, S. Finck, R. Ros, and A. Auger (2009).
<a class="reference external" href="http://coco.gforge.inria.fr/">Real-parameter black-box optimization benchmarking 2009: Noiseless functions definitions</a>. <a class="reference external" href="https://hal.inria.fr/inria-00362633">Research Report RR-6829</a>, Inria, updated February 2010.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="han2009noi" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id61">[HAN2009noi]</a></td><td>N. Hansen, S. Finck, R. Ros, and A. Auger (2009).
<a class="reference external" href="http://coco.gforge.inria.fr/">Real-Parameter Black-Box Optimization Benchmarking 2009: Noisy Functions Definitions</a>. <a class="reference external" href="https://hal.inria.fr/inria-00369466">Research Report RR-6869</a>, Inria, updated February 2010.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="han2016ex" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[HAN2016ex]</td><td><em>(<a class="fn-backref" href="#id13">1</a>, <a class="fn-backref" href="#id21">2</a>, <a class="fn-backref" href="#id33">3</a>, <a class="fn-backref" href="#id42">4</a>)</em> N. Hansen, T. Tušar, A. Auger, D. Brockhoff, O. Mersmann (2016).
<a class="reference external" href="http://numbbo.github.io/coco-doc/experimental-setup/">COCO: Experimental Procedure</a>, <em>ArXiv e-prints</em>, <a class="reference external" href="http://arxiv.org/abs/1603.08776">arXiv:1603.08776</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="hun2007" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id59">[HUN2007]</a></td><td>J. D. Hunter (2007). <a class="reference external" href="http://matplotlib.org/">Matplotlib</a>: A 2D graphics environment,
<em>Computing In Science &amp; Engineering</em>, 9(3): 90-95.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="efr1994" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id46">[EFR1994]</a></td><td>B. Efron and R. Tibshirani (1994). <em>An introduction to the
bootstrap</em>. CRC Press.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="har1999" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id40">[HAR1999]</a></td><td>G. R. Harik and F. G. Lobo (1999). A parameter-less genetic
algorithm. In <em>Proceedings of the Genetic and Evolutionary Computation
Conference (GECCO)</em>, volume 1, pages 258-265. ACM.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="hoo1998" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id23">[HOO1998]</a></td><td>H. H. Hoos and T. Stützle (1998). Evaluating Las Vegas
algorithms: pitfalls and remedies. In <em>Proceedings of the Fourteenth
Conference on Uncertainty in Artificial Intelligence (UAI-98)</em>,
pages 238-245.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="mor2009" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id51">[MOR2009]</a></td><td>J. Moré and S. Wild (2009).
Benchmarking Derivative-Free Optimization Algorithms. <em>SIAM J. Optimization</em>, 20(1):172-191.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="mor2002" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id52">[MOR2002]</a></td><td>D. Dolan and J. J. Moré (2002).
Benchmarking Optimization Software with Performance Profiles. <em>Mathematical Programming</em>, 91:201-213.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="ste1946" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[STE1946]</td><td><em>(<a class="fn-backref" href="#id8">1</a>, <a class="fn-backref" href="#id16">2</a>)</em> S.S. Stevens (1946).
On the theory of scales of measurement. <em>Science</em> 103(2684), pp. 677-680.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="tus2016" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[TUS2016]</td><td><em>(<a class="fn-backref" href="#id31">1</a>, <a class="fn-backref" href="#id62">2</a>)</em> T. Tušar, D. Brockhoff, N. Hansen, A. Auger (2016).
<a class="reference external" href="http://numbbo.github.io/coco-doc/bbob-biobj/functions/">COCO: The Bi-objective Black Box Optimization Benchmarking (bbob-biobj)
Test Suite</a>, <em>ArXiv e-prints</em>, <a class="reference external" href="http://arxiv.org/abs/1604.00359">arXiv:1604.00359</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="whi1996" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[WHI1996]</td><td><em>(<a class="fn-backref" href="#id12">1</a>, <a class="fn-backref" href="#id19">2</a>)</em> D. Whitley, S. Rana, J. Dzubera, K. E. Mathias (1996).
Evaluating evolutionary algorithms. <em>Artificial intelligence</em>, 85(1), 245-276.</td></tr>
</tbody>
</table>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="nav-item nav-item-0"><a href="#">COCO: COmparing Continuous Optimizers</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
      Last updated on May 12, 2016.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.3.5.
    </div>
  </body>
</html>